{"plain_text": "Mercator: A Scalable, ExtensibleWeb CrawlerAllan Heydon and Marc NajorkCompaq Systems Research Center130 Lytton AvenuePalo Alto, CA 94301{heydon,najork}@pa.dec.comJune 26, 1999AbstractThis paper describes Mercator, a scalable, extensible web crawler written entirely in Java. Scalable web\ncrawlers are an important component of many web services, but their design is not well-documented in the\nliterature. We enumerate the major components of any scalable web crawler, comment on alternatives and\ntradeoffs in their design, and describe the particular components used in Mercator. We also describe Mercator's\nsupport for extensibility and customizability. Finally, we comment on Mercator's performance, which we have\nfound to be comparable to that of other crawlers for which performance numbers have been published.1. IntroductionDesigning a scalable web crawler comparable to the ones used by the major search engines is a complex endeavor.\nHowever, due to the competitive nature of the search engine business, there are few papers in the literature describing the\nchallenges and tradeoffs inherent in web crawler design. This paper's main contribution is to fill that gap. It describes\nMercator, a scalable, extensible web crawler written entirely in Java.\nBy scalable, we mean that Mercator is designed to scale up to the entire web, and has been used to fetch tens of millions ofweb documents. We achieve scalability by implementing our data structures so that they use a bounded amount of memory,\nregardless of the size of the crawl. Hence, the vast majority of our data structures are stored on disk, and small parts of them\nare stored in memory for efficiency.\nBy extensible, we mean that Mercator is designed in a modular way, with the expectation that new functionality will beadded by third parties. In practice, it has been used to create a snapshot of the web pages on our corporate intranet, to collect\na variety of statistics about the web, and to perform a series of random walks of the web\u20ac[12].One of the initial motivations of this work was to collect statistics about the web. There are many statistics that might be of\ninterest, such as the size and the evolution of the URL space, the distribution of web servers over top-level domains, the\nlifetime and change rate of documents, and so on. However, it is hard to know a priori exactly which statistics areinteresting, and topics of interest may change over time. Mercator makes it easy to collect new statistics --- or more\ngenerally, to be configured for different crawling tasks --- by allowing users to provide their own modules for processing\ndownloaded documents. For example, when we designed Mercator, we did not anticipate the possibility of using it for the\nrandom walk application cited above. Despite the differences between random walking and traditional crawling, we were\nable to reconfigure Mercator as a random walker without modifying the crawler's core, merely by plugging in modules\ntotaling 360 lines of Java source code.The remainder of the paper is structured as follows. The next section surveys related work. Section\u20ac3 describes the maincomponents of a scalable web crawler, the alternatives and tradeoffs in their design, and the particular choices we made inMercator. Section\u20ac4 describes Mercator's support for extensibility. Section\u20ac5 describes some the web crawling problems thatMercator: A Scalable, Extensible Web Crawler\nhttp://www.research.compaq.com/SRC/mercator/papers/www/paper.html (1 de 14) [29/10/2003 16:33:31]arise due to the inherent anarchy of the web. Section\u20ac6 reports on performance measurements and web statistics collectedduring a recent extended crawl. Finally, Section\u20ac7 offers our conclusions.2. Related WorkWeb crawlers --- also known as robots, spiders, worms, walkers, and wanderers --- are almost as old as the web itself\u20ac[15].The first crawler, Matthew Gray's Wanderer, was written in the spring of 1993, roughly coinciding with the first release ofNCSA Mosaic\u20ac[22]. Several papers about web crawling were presented at the first two World Wide Webconferences\u20ac[9,16,18]. However, at the time, the web was two to three orders of magnitude smaller than it is today, so thosesystems did not address the scaling problems inherent in a crawl of today's web.\nObviously, all of the popular search engines use crawlers that must scale up to substantial portions of the web. However, due\nto the competitive nature of the search engine business, the designs of these crawlers have not been publicly described.\nThere are two notable exceptions: the Google crawler and the Internet Archive crawler. Unfortunately, the descriptions of\nthese crawlers in the literature are too terse to enable reproducibility.The Google search engine is a distributed system that uses multiple machines for crawling\u20ac[4,11]. The crawler consists offive functional components running in different processes. A URL server process reads URLs out of a file and forwardsthem to multiple crawler processes. Each crawler process runs on a different machine, is single-threaded, and usesasynchronous I/O to fetch data from up to 300 web servers in parallel. The crawlers transmit downloaded pages to a single\nStoreServer process, which compresses the pages and stores them to disk. The pages are then read back from disk by anindexer process, which extracts links from HTML pages and saves them to a different disk file. A URL resolver processreads the link file, derelativizes the URLs contained therein, and saves the absolute URLs to the disk file that is read by the\nURL server. Typically, three to four crawler machines are used, so the entire system requires between four and eight\nmachines.The Internet Archive also uses multiple machines to crawl the web\u20ac[6,14]. Each crawler process is assigned up to 64 sites tocrawl, and no site is assigned to more than one crawler. Each single-threaded crawler process reads a list of seed URLs for\nits assigned sites from disk into per-site queues, and then uses asynchronous I/O to fetch pages from these queues in parallel.\nOnce a page is downloaded, the crawler extracts the links contained in it. If a link refers to the site of the page it was\ncontained in, it is added to the appropriate site queue; otherwise it is logged to disk. Periodically, a batch process merges\nthese logged ``cross-site'' URLs into the site-specific seed sets, filtering out duplicates in the process.In the area of extensible web crawlers, Miller and Bharat's SPHINX system\u20ac[17] provides some of the same customizabilityfeatures as Mercator. In particular, it provides a mechanism for limiting which pages are crawled, and it allows customized\ndocument processing code to be written. However, SPHINX is targeted towards site-specific crawling, and therefore is not\ndesigned to be scalable.3. Architecture of a Scalable Web CrawlerThe basic algorithm executed by any web crawler takes a list of seed URLs as its input and repeatedly executes thefollowing steps. Remove a URL from the URL list, determine the IP address of its host name, download the corresponding\ndocument, and extract any links contained in it. For each of the extracted links, ensure that it is an absolute URL\n(derelativizing it if necessary), and add it to the list of URLs to download, provided it has not been encountered before. If\ndesired, process the downloaded document in other ways (e.g., index its content). This basic algorithm requires a number of\nfunctional components:a component (called the URL frontier) for storing the list of URLs to download;l   a component for resolving host names into IP addresses;l   a component for downloading documents using the HTTP protocol;l   a component for extracting links from HTML documents; andl   a component for determining whether a URL has been encountered before.l   Mercator: A Scalable, Extensible Web Crawler\nhttp://www.research.compaq.com/SRC/mercator/papers/www/paper.html (2 de 14) [29/10/2003 16:33:31]This section describes how Mercator refines this basic algorithm, and the particular implementations we chose for the\nvarious components. Where appropriate, we comment on design alternatives and the tradeoffs between them.3.1. Mercator's ComponentsFigure\u20ac1 shows Mercator's main components. Crawling is performed by multiple worker threads, typically numbering in the\nhundreds. Each worker repeatedly performs the steps needed to download and process a document. The first step of this loop is to remove an absolute URL from the shared URL frontier for downloading.Figure 1: Mercator's main components.An absolute URL begins with a scheme (e.g., ``http''), which identifies the network protocol that should be used to downloadit. In Mercator, these network protocols are implemented by protocol modules. The protocol modules to be used in a crawlare specified in a user-supplied configuration file, and are dynamically loaded at the start of the crawl. The default\nconfiguration includes protocol modules for HTTP, FTP, and Gopher. As suggested by the tiling of the protocol modules in\nFigure 1, there is a separate instance of each protocol module per thread, which allows each thread to access local data\nwithout any synchronization.\nBased on the URL's scheme, the worker selects the appropriate protocol module for downloading the document. It then\ninvokes the protocol module's fetch method, which downloads the document from the Internet  into a per-threadRewindInputStream  (or RIS for short). A RIS is an I/O abstraction that is initialized from an arbitrary input stream, andthat subsequently allows that stream's contents to be re-read multiple times.\nOnce the document has been written to the RIS, the worker thread invokes the content-seen test to determine whether thisdocument (associated with a different URL) has been seen before . If so, the document is not processed any further, andthe worker thread removes the next URL from the frontier.\nEvery downloaded document has an associated MIME type. In addition to associating schemes with protocol modules, a\nMercator configuration file also associates MIME types with one or more processing modules. A processing module is anabstraction for processing downloaded documents, for instance extracting links from HTML pages, counting the tags found\nin HTML pages, or collecting statistics about GIF images. Like protocol modules, there is a separate instance of each\nprocessing module per thread. In general, processing modules may have side-effects on the state of the crawler, as well as on\ntheir own internal state.\nBased on the downloaded document's MIME type, the worker invokes the process method of each processing moduleassociated with that MIME type . For example, the Link Extractor and Tag Counter processing modules in Figure 1 areMercator: A Scalable, Extensible Web Crawler\nhttp://www.research.compaq.com/SRC/mercator/papers/www/paper.html (3 de 14) [29/10/2003 16:33:31]used for text/html documents, and the GIF Stats module is used for image/gif documents.\nBy default, a processing module for extracting links is associated with the MIME type text/html. The process method of thismodule extracts all links from an HTML page. Each link is converted into an absolute URL, and tested against a\nuser-supplied URL filter to determine if it should be downloaded . If the URL passes the filter, the worker performs theURL-seen test , which checks if the URL has been seen before, namely, if it is in the URL frontier or has already beendownloaded. If the URL is new, it is added to the frontier .The above description omits several important implementation details. Designing data structures that can efficiently handle\nhundreds of millions of entries poses many engineering challenges. Central to these concerns are the need to balance\nmemory use and performance. In the remainder of this section, we describe how we address this time-space tradeoff in\nseveral of Mercator's main data structures.3.2. The URL FrontierThe URL frontier is the data structure that contains all the URLs that remain to be downloaded. Most crawlers work by\nperforming a breath-first traversal of the web, starting from the pages in the seed set. Such traversals are easily implemented\nby using a FIFO queue.\nIn a standard FIFO queue, elements are dequeued in the order they were enqueued. In the context of web crawling, however,\nmatters are complicated by the fact that it is considered socially unacceptable to have multiple HTTP requests pending to the\nsame server. If multiple requests are to be made in parallel, the queue's remove operation should not simply return the headof the queue, but rather a URL close to the head whose host has no outstanding request.\nTo implement this politeness constraint, the default version of Mercator's URL frontier is actually implemented by a\ncollection of distinct FIFO subqueues. There are two important aspects to how URLs are added to and removed from these\nqueues. First, there is one FIFO subqueue per worker thread. That is, each worker thread removes URLs from exactly one of\nthe FIFO subqueues. Second, when a new URL is added, the FIFO subqueue in which it is placed is determined by the\nURL's canonical host name. Together, these two points imply that at most one worker thread will download documents from\na given web server. This design prevents Mercator from overloading a web server, which could otherwise become a\nbottleneck of the crawl.\nIn actual world wide web crawls, the size of the crawl's frontier numbers in the hundreds of millions of URLs. Hence, the\nmajority of the URLs must be stored on disk. To amortize the cost of reading from and writing to disk, our FIFO subqueue\nimplementation maintains fixed-size enqueue and dequeue buffers in memory; our current implementation uses buffers that\ncan hold 600 URLs each.As described in Section\u20ac4 below, the URL frontier is one of Mercator's ``pluggable'' components, meaning that it can easilybe replaced by other implementations. For example, one could implement a URL frontier that uses a ranking of URLimportance (such as the PageRank metric\u20ac[4]) to order URLs in the frontier set. Cho, Garcia-Molina, and Page haveperformed simulated crawls showing that such ordering can improve crawling effectiveness\u20ac[7].3.3. The HTTP Protocol ModuleThe purpose of a protocol module is to fetch the document corresponding to a given URL using the appropriate network\nprotocol. Network protocols supported by Mercator include HTTP, FTP, and Gopher.\nCourteous web crawlers implement the Robots Exclusion Protocol, which allows web masters to declare parts of their sitesoff limits to crawlers\u20ac[20]. The Robots Exclusion Protocol requires a web crawler to fetch a special document containingthese declarations from a web site before downloading any real content from it. To avoid downloading this file on every\nrequest, Mercator's HTTP protocol module maintains a fixed-sized cache mapping host names to their robots exclusion\nrules. By default, the cache is limited to 218 entries, and uses an LRU replacement strategy.Our initial HTTP protocol module used the HTTP support provided by the JDK 1.1 Java class libraries. However, we soon\ndiscovered that this implementation did not allow us to specify any timeout values on HTTP connections, so a maliciousMercator: A Scalable, Extensible Web Crawler\nhttp://www.research.compaq.com/SRC/mercator/papers/www/paper.html (4 de 14) [29/10/2003 16:33:31]web server could cause a worker thread to hang indefinitely. Also, the JDK implementation was not particularly efficient.\nTherefore, we wrote our own ``lean and mean'' HTTP protocol module; its requests time out after 1 minute, and it has\nminimal synchronization and allocation overhead.3.4. Rewind Input StreamMercator's design allows the same document to be processed by multiple processing modules. To avoid reading a document\nover the network multiple times, we cache the document locally using an abstraction called a RewindInputStream (RIS).A RIS is an input stream with an open method that reads and caches the entire contents of a supplied input stream (such asthe input stream associated with a socket). A RIS caches small documents (64 KB or less) entirely in memory, while larger\ndocuments are temporarily written to a backing file. The RIS constructor allows a client to specify an upper limit on the size\nof the backing file as a safeguard against malicious web servers that might return documents of unbounded size. By default,\nMercator sets this limit to 1 MB. In addition to the functionality provided by normal input streams, a RIS also provides a\nmethod for rewinding its position to the beginning of the stream, and various lexing methods that make it easy to build\nMIME-type-specific parsers.\nEach worker thread has an associated RIS, which it reuses from document to document. After removing a URL from the\nfrontier, a worker passes that URL to the appropriate protocol module, which initializes the RIS from a network connection\nto contain the document's contents. The worker then passes the RIS to all relevant processing modules, rewinding the stream\nbefore each module is invoked.3.5. Content-Seen TestMany documents on the web are available under multiple, different URLs. There are also many cases in which documents\nare mirrored on multiple servers. Both of these effects will cause any web crawler to download the same document contents\nmultiple times. To prevent processing a document more than once, a web crawler may wish to perform a content-seen test todecide if the document has already been processed. Using a content-seen test makes it possible to suppress link extraction\nfrom mirrored pages, which may result in a significant reduction in the number of pages that need to be downloaded(Section\u20ac5 below elaborates on these points). Mercator includes just such a content-seen test, which also offers the sidebenefit of allowing us to keep statistics about the fraction of downloaded documents that are duplicates of pages that have\nalready been downloaded.\nThe content-seen test would be prohibitively expensive in both space and time if we saved the complete contents of every\ndownloaded document. Instead, we maintain a data structure called the document fingerprint set that stores a 64-bitchecksum of the contents of each downloaded document. We compute the checksum using Broder's implementation\u20ac[5] ofRabin's fingerprinting algorithm\u20ac[19]. Fingerprints offer provably strong probabilistic guarantees that two different stringswill not have the same fingerprint. Other checksum algorithms, such as MD5 and SHA, do not offer such provable\nguarantees, and are also more expensive to compute than fingerprints.\nWhen crawling the entire web, the document fingerprint set will obviously be too large to be stored entirely in memory.\nUnfortunately, there is very little locality in the requests made on the document fingerprint set, so caching such requests has\nlittle benefit. We therefore maintain two independent sets of fingerprints: a small hash table kept in memory, and a large\nsorted list kept in a single disk file.\nThe content-seen test first checks if the fingerprint is contained in the in-memory table. If not, it has to check if the\nfingerprint resides in the disk file. To avoid multiple disk seeks and reads per disk search, Mercator performs an interpolated\nbinary search of an in-memory index of the disk file to identify the disk block on which the fingerprint would reside if it\nwere present. It then searches the appropriate disk block, again using interpolated binary search. We use a buffered variant\nof Java's random access files, which guarantees that searching through one disk block causes at most two kernel calls (one\nseek and one read). We use a customized data structure instead of a more generic data structure such as a B-tree because ofthis guarantee. It is worth noting that Mercator's ability to be dynamically configured (see Section\u20ac4 below) would easilyallow someone to replace our implementation with a different one based on B-trees.\nWhen a new fingerprint is added to the document fingerprint set, it is added to the in-memory table. When this table fills up,\nits contents are merged with the fingerprints on disk, at which time the in-memory index of the disk file is updated as well.Mercator: A Scalable, Extensible Web Crawler\nhttp://www.research.compaq.com/SRC/mercator/papers/www/paper.html (5 de 14) [29/10/2003 16:33:31]To guard against races, we use a readers-writer lock that controls access to the disk file. Threads must hold a read share of\nthe lock while reading from the file, and must hold the write lock while writing to it.3.6. URL FiltersThe URL filtering mechanism provides a customizable way to control the set of URLs that are downloaded. Before adding a\nURL to the frontier, the worker thread consults the user-supplied URL filter. The URL filter class has a single crawl methodthat takes a URL and returns a boolean value indicating whether or not to crawl that URL. Mercator includes a collection of\ndifferent URL filter subclasses that provide facilities for restricting URLs by domain, prefix, or protocol type, and for\ncomputing the conjunction, disjunction, or negation of other filters. Users may also supply their own custom URL filters,\nwhich are dynamically loaded at start-up.3.7. Domain Name ResolutionBefore contacting a web server, a web crawler must use the Domain Name Service (DNS) to map the web server's host\nname into an IP address. DNS name resolution is a well-documented bottleneck of most web crawlers. This bottleneck is\nexacerbated in any crawler, like Mercator or the Internet Archive crawler, that uses DNS to canonicalize the host names of\nnewly discovered URLs before performing the URL-seen test on them.\nWe tried to alleviate the DNS bottleneck by caching DNS results, but that was only partially effective. After some probing,\nwe discovered that the Java interface to DNS lookups is synchronized. Further investigation revealed that the DNS interface\non most flavors of Unix (i.e., the gethostbyname function provided as part of the Berkeley Internet Name Domain (BIND)distribution\u20ac[2]) is also synchronized1. This meant that only one DNS request on an uncached name could be outstanding atonce. The cache miss rate is high enough that this limitation causes a bottleneck.\nTo work around these problems, we wrote our own multi-threaded DNS resolver class and integrated it into Mercator. The\nresolver forwards DNS requests to a local name server, which does the actual work of contacting the authoritative server for\neach query. Because multiple requests can be made in parallel, our resolver can resolve host names much more rapidly than\neither the Java or Unix resolvers.\nThis change led to a significant crawling speedup. Before making the change, performing DNS lookups accounted for 87%\nof each thread's elapsed time. Using our custom resolver reduced that elapsed time to 25%. (Note that the actual number of\nCPU cycles spent on DNS resolution is extremely low. Most of the elapsed time is spent waiting on remote DNS servers.)\nMoreover, because our resolver can perform resolutions in parallel, DNS is no longer a bottleneck; if it were, we would\nsimply increase the number of worker threads.3.8. URL-Seen TestIn the course of extracting links, any web crawler will encounter multiple links to the same document. To avoid\ndownloading and processing a document multiple times, a URL-seen test must be performed on each extracted link before\nadding it to the URL frontier. (An alternative design would be to instead perform the URL-seen test when the URL is\nremoved from the frontier, but this approach would result in a much larger frontier.)\nTo perform the URL-seen test, we store all of the URLs seen by Mercator in canonical form in a large table called the URLset. Again, there are too many entries for them all to fit in memory, so like the document fingerprint set, the URL set isstored mostly on disk.\nTo save space, we do not store the textual representation of each URL in the URL set, but rather a fixed-sized checksum.\nUnlike the fingerprints presented to the content-seen test's document fingerprint set, the stream of URLs tested against the\nURL set has a non-trivial amount of locality. To reduce the number of operations on the backing disk file, we therefore keep\nan in-memory cache of popular URLs. The intuition for this cache is that links to some URLs are quite common, so caching\nthe popular ones in memory will lead to a high in-memory hit rate.\nIn fact, using an in-memory cache of 218 entries and the LRU-like clock replacement policy, we achieve an overall hit rateon the in-memory cache of 66.2%, and a hit rate of 9.5% on the table of recently-added URLs, for a net hit rate of 75.7%.\nMoreover, of the 24.3% of requests that miss in both the cache of popular URLs and the table of recently-added URLs,Mercator: A Scalable, Extensible Web Crawler\nhttp://www.research.compaq.com/SRC/mercator/papers/www/paper.html (6 de 14) [29/10/2003 16:33:31]about 1/3 produce hits on the buffer in our random access file implementation, which also resides in user-space. The net\nresult of all this buffering is that each membership test we perform on the URL set results in an average of 0.16 seek and0.17 read kernel calls (some fraction of which are served out of the kernel's file system buffers). So, each URL setmembership test induces one-sixth as many kernel calls as a membership test on the document fingerprint set. These savings\nare purely due to the amount of URL locality (i.e., repetition of popular URLs) inherent in the stream of URLs encountered\nduring a crawl.\nHowever, due to the prevalence of relative URLs in web pages, there is a second form of locality in the stream of discovered\nURLs, namely, host name locality. Host name locality arises because many links found in web pages are to different\ndocuments on the same server. Unfortunately, computing a URL's checksum by simply fingerprinting its textual\nrepresentation would cause this locality to be lost. To preserve the locality, we compute the checksum of a URL by merging\ntwo independent fingerprints: one of the URL's host name, and the other of the complete URL. These two fingerprints are\nmerged so that the high-order bits of the checksum derive from the host name fingerprint. As a result, checksums for URLs\nwith the same host component are numerically close together. So, the host name locality in the stream of URLs translates\ninto access locality on the URL set's backing disk file, thereby allowing the kernel's file system buffers to service read\nrequests from memory more often. On extended crawls in which the size of the URL set's backing disk file significantly\nexceeds the size of the kernel's file system buffers, this technique results in a significant reduction in disk load, and hence, in\n\na significant performance improvement.The Internet Archive crawler implements the URL-seen test using a different data structure called a bloom filter\u20ac[3]. Abloom filter is a probabilistic data structure for set membership testing that may yield false positives. The set is represented\nby a large bit vector. An element is added to the set by computing n hash functions of the element and setting thecorresponding bits. An element is deemed to be in the set if the bits at all n of the element's hash locations are set. Hence, adocument may incorrectly be deemed to be in the set, but false negatives are not possible.\nThe disadvantage to using a bloom filter for the URL-seen test is that each false positive will cause the URL not to be added\nto the frontier, and therefore the document will never be downloaded. The chance of a false positive can be reduced by\nmaking the bit vector larger. The Internet Archive crawler uses 10 hash functions and a separate 32 KB bit vector for each of\nthe sites currently being crawled. For the batch phase in which non-local URLs are merged, a much larger 2 GB bit vector is\nused. As the web grows, the batch process will have to be run on a machine with larger amounts of memory, or disk-based\ndata structures will have to be used. By contrast, our URL-seen test does not admit false positives, and it uses a bounded\namount of memory, regardless of the size of the web.3.9. Synchronous vs. Asynchronous I/OBoth the Google and Internet Archive crawlers use single-threaded crawling processes and asynchronous I/O to perform\nmultiple downloads in parallel. In contrast, Mercator uses a multi-threaded process in which each thread performs\nsynchronous I/O. These two techniques are different means for achieving the same effect.\nThe main advantage to using multiple threads and synchronous I/O is that it leads to a much simpler program structure.\nSwitching between threads of control is left to the operating system's thread scheduler, rather than having to be coded in the\nuser program. The sequence of tasks executed by each worker thread is much more straightforward and self-evident. One\nstrength of the Google and the Internet Archive crawlers is that they are designed from the ground up to scale to multiple\nmachines. However, whether a crawler is distributed or not is orthogonal to the choice between synchronous and\nasynchronous I/O. It would not be too difficult to adapt Mercator to run on multiple machines, while still using synchronous\nI/O and multiple threads within each process.3.10. CheckpointingA crawl of the entire web takes weeks to complete. To guard against failures, Mercator writes regular snapshots of its state\nto disk. These snapshots are orthogonal to Mercator's other disk-based data structures. An interrupted or aborted crawl can\neasily be restarted from the latest checkpoint. We define a general checkpointing interface that is implemented by the\nMercator classes constituting the crawler core. User-supplied protocol or processing modules are also required to implement\nthe checkpointing interface. Checkpoints are coordinated using a global readers-writer lock. Each worker thread acquires a\nread share of the lock while processing a downloaded document. At regular intervals, typically once a day, Mercator's mainMercator: A Scalable, Extensible Web Crawler\nhttp://www.research.compaq.com/SRC/mercator/papers/www/paper.html (7 de 14) [29/10/2003 16:33:31]thread acquires the write lock, so it is guaranteed to be running in isolation. Once it has acquired the lock, the main thread\narranges for the checkpoint methods to be called on Mercator's core classes and all user-supplied modules.4. ExtensibilityAs mentioned previously, Mercator is an extensible crawler. In practice, this means two things. First, Mercator can be\nextended with new functionality. For example, new protocol modules may be provided for fetching documents according to\ndifferent network protocols, or new processing modules may be provided for processing downloaded documents in\ncustomized ways. Second, Mercator can easily be reconfigured to use different versions of most of its major components. Inparticular, different versions of the URL frontier (Section\u20ac3.2), document fingerprint set (Section\u20ac3.5), URL filter(Section\u20ac3.6), and URL set (Section\u20ac3.8) may all be dynamically ``plugged into'' the crawler. In fact, we have writtenmultiple versions of each of these components, which we employ for different crawling tasks.\nMaking an extensible system such as Mercator requires three ingredients:The interface to each of the system's components must be well-specified. In Mercator, the interface to each\ncomponent is defined by an abstract class, some of whose methods are also abstract. Any component implementationis required to be a subclass of this abstract class that provides implementations for the abstract methods.l   A mechanism must exist for specifying how the crawler is to be configured from its various components. In Mercator,\nthis is done by supplying a configuration file to the crawler when it starts up. Among other things, the configurationfile specifies which additional protocol and processing modules should be used, as well as the concrete\nimplementation to use for each of the crawler's ``pluggable'' components. When Mercator is started, it reads the\nconfiguration file, uses Java's dynamic class loading feature to dynamically instantiate the necessary components,\nplugs these instances into the appropriate places in the crawler core's data structures, and then begins the crawl.\nSuitable defaults are defined for all components.l   Sufficient infrastructure must exist to make it easy to write new components. In Mercator, this infrastructure consists\nof a rich set of utility libraries together with a set of existing pluggable components. An object-oriented language such\nas Java makes it easy to construct a new component by subclassing an existing component and overriding some of its\nmethods. For example, our colleague Mark Manasse needed information about the distribution of HTTP 401 return\ncodes across web servers. He was able to obtain this data by subclassing Mercator's default HTTP protocol\ncomponent, and by using one of our standard histogram classes to maintain the statistics. As a result, his custom\nHTTP protocol component required only 58 lines of Java source code.l   To demonstrate Mercator's extensibility, we now describe some of the extensions we've written.4.1. Protocol and Processing ModulesBy default, Mercator will crawl the web by fetching documents using the HTTP protocol, extracting links from documents\nof type text/html. However, aside from extracting links, it does not process the documents in any way. To fetch documents\nusing additional protocols or to process the documents once they are fetched, new protocol and processing modules must be\nsupplied.\nThe abstract Protocol class includes only two methods. The fetch method downloads the document corresponding to a givenURL, and the newURL method parses a given string, returning a structured URL object. The latter method is necessarybecause URL syntax varies from protocol to protocol. In addition to the HTTP protocol module, we have also written\nprotocol modules for fetching documents using the FTP and Gopher protocols.\nThe abstract Analyzer class is the superclass for all processing modules. It defines a single process method. This method isresponsible for reading the document and processing it appropriately. The method may make method calls on other parts of\nthe crawler (e.g., to add newly discovered URLs to the frontier). Analyzers often keep private state or write data to the disk.\nWe have written a variety of different Analyzer subclasses. Some of these analyzers keeps statistics, such as our GifStats andTagCounter subtypes. Other processing modules simply write the contents of each downloaded document to disk. Ourcolleague Raymie Stata has written such a module; the files it writes are in a form suitable to be read by the AltaVista\nindexing software. As another experiment, we wrote a WebLinter processing module that runs the weblint program on eachMercator: A Scalable, Extensible Web Crawler\nhttp://www.research.compaq.com/SRC/mercator/papers/www/paper.html (8 de 14) [29/10/2003 16:33:31]downloaded HTML page to check it for errors, logging all discovered errors to a file. These processing modules range in\nsize from 70 to 270 lines of Java code, including comments; the majority are less than 100 lines long.4.2. Alternative URL Frontier ImplementationIn Section\u20ac3.2, we described one implementation of the URL frontier data structure. However, when we ran thatimplementation on a crawl of our corporate intranet, we discovered that it had the drawback that multiple hosts might be\nassigned to the same worker thread, while other threads were left idle. This situation is more likely to occur on an intranet\nbecause intranets typically contain a substantially smaller number of hosts than the internet at large.\nTo restore the parallelism lost by our initial URL frontier implementation, we wrote an alternative URL frontier component\nthat dynamically assigns hosts to worker threads. Like our initial implementation, the second version guarantees that at most\none worker thread will download documents from any given web server at once; moreover, it maximizes the number of busy\nworker threads within the limits of this guarantee. In particular, all worker threads will be busy so long as the number of\ndifferent hosts in the frontier is at least the number of worker threads. The second version is well-suited to host-limited\ncrawls (such as intranet crawls), while the initial version is preferable for internet crawls, since it does not have the\noverheads required by the second version to maintain a dynamic mapping from host names to worker threads.4.3. Configuring Mercator as a Random WalkerWe have used Mercator to perform random walks of the web in order to gather a sample of web pages; the sampled pageswere used to measure the quality of search engines\u20ac[12]. A random walk starts at a random page taken from a set of seeds.The next page to fetch is selected by choosing a random link from the current page. The process continues until it arrives at\na page with no links, at which time the walk is restarted from a new random seed page. The seed set is dynamically extended\nby the newly discovered pages, and cycles are broken by performing random restarts every so often.\nPerforming a random walk of the web is quite different from an ordinary crawl for two reasons. First, a page may be\nrevisited multiple times during the course of a random walk. Second, only one link is followed each time a page is visited. In\norder to support random walking, we wrote a new URL frontier class that does not maintain a set of all added URLs, but\ninstead records only the URLs discovered on each thread's most recently fetched page. Its remove method selects one ofthese URLs at random and returns it. To allow pages to be processed multiple times, we also replaced the document\nfingerprint set by a new version that never rejects documents as already having been seen. Finally, we subclassed the default\nLinkExtractor class to perform extra logging. The new classes are ``plugged into'' the crawler core at runtime using theextension mechanism described above. In total, the new classes required for random walking amount to 360 lines of Java\nsource code.5. Crawler Traps and Other HazardsIn the course of our experiments, we had to overcome several pitfalls that would otherwise cause Mercator to download\nmore documents than necessary. Although the web contains a finite number of static documents (i.e., documents that are not\ngenerated on-the-fly), there are an infinite number of retrievable URLs. Three frequent causes of this inflation are URL\naliases, session IDs embedded in URLs, and crawler traps. This section describes those problems and the techniques we use\nto avoid them.5.1. URL AliasesTwo URLs are aliases for each other if they refer to the same content. There are four causes of URL aliases:Host name aliases\nHost name aliases occur when multiple host names correspond to the same IP address. For example, the host names\ncoke.com and cocacola.com both correspond to the host whose IP address is 208.134.241.178. Hence, everydocument served by that machine's web server has at least three different URLs (one using each of the two host\nnames, and one using the IP address).l   Mercator: A Scalable, Extensible Web Crawler\nhttp://www.research.compaq.com/SRC/mercator/papers/www/paper.html (9 de 14) [29/10/2003 16:33:31]Before performing the URL-seen test, we canonicalize a host name by issuing a DNS request containing a CNAME\n(``canonical name'') and an A (``addresses'') query. We use the host's canonical name if one is returned; otherwise, we\nuse the smallest returned IP address.\nIt should be mentioned that this technique may be too aggressive for some virtual domains. Many Internet serviceproviders (ISPs) serve up multiple domains from the same web server, using the ``Host'' header field of the HTTP\nrequest to recover the host portion of the URL. If an ISP does not provide distinct CNAME records for these virtual\ndomains, URLs from these domains will be collapsed into the same host space by our canonicalization process. In\npractice, however, most ISPs seem to return adequate CNAME entries.Omitted port numbers\nIf a port number is not specified in a URL, a protocol-specific default value is used. We therefore insert the default\nvalue, such as 80 in the case of HTTP, before performing the URL-seen test on it.l   Alternative paths on the same host\nOn a given host, there may be multiple paths to the same file. For example, the two URLs\nhttp://www.digital.com/index.html and http://www.digital.com/home.html both refer to the same file. One commoncause of this phenomenon is the use of symbolic links within the server machine's file system.l   Replication across different hosts\nFinally, multiple copies of a document may reside on different web servers. Mirror sites are a common instance of this\nphenomenon, as are multiple web servers that access the same shared file system.l   In the latter two cases, we cannot avoid downloading duplicate documents. However, we do avoid processing all but the firstcopy by using the content-seen test described in Section\u20ac3.5. During an eight-day crawl described in Section\u20ac6 below, 8.5%of the documents we fetched were duplicates. Had we not performed the content-seen test, the number of unnecessary\ndownloads would have been much higher, since we would have followed links from duplicate documents to other\ndocuments that are likely to be duplicates as well.5.2. Session IDs Embedded in URLsTo track the browsing behavior of their visitors, a number of web sites embed dynamically-generated session identifiers\nwithin the links of returned pages. Session IDs create a potentially infinite set of URLs that refer to the same document.\nWith the advent of cookies, the use of embedded session IDs has diminished, but they are still used, partly because most\nbrowsers have a facility for disabling cookies.\nEmbedded session IDs represent a special case of alternative paths on the same host as described in the previous section.\nThus, Mercator's document fingerprinting technique prevents excessive downloading due to embedded session IDs, although\nit is not powerful enough to automatically detect and remove them.5.3. Crawler TrapsA crawler trap is a URL or set of URLs that cause a crawler to crawl indefinitely. Some crawler traps are unintentional. Forexample, a symbolic link within a file system can create a cycle. Other crawler traps are introduced intentionally. For\nexample, people have written traps using CGI programs that dynamically generate an infinite web of documents. The\nmotivations behind such traps vary. Anti-spam traps are designed to catch crawlers used by ``Internet marketeers'' (better\nknown as spammers) looking for e-mail addresses, while other sites use traps to catch search engine crawlers so as to boost\ntheir search ratings.\nWe know of no automatic technique for avoiding crawler traps. However, sites containing crawler traps are easily noticed\ndue to the large number of documents discovered there. A human operator can verify the existence of a trap and manually\nexclude the site from the crawler's purview using the customizable URL filter described in Section 3.6.Mercator: A Scalable, Extensible Web Crawler\nhttp://www.research.compaq.com/SRC/mercator/papers/www/paper.html (10 de 14) [29/10/2003 16:33:31]6. Results of an Extended CrawlThis section reports on Mercator's performance during an eight-day crawl, and presents some statistics about the web\ncollected during that crawl. In our analysis of Mercator's performance, we contrast it with the performance of the Google\nand Internet Archive crawlers. We make no attempt to adjust for different hardware configurations since the papers\ndescribing the two other crawlers do not contain enough information to do so. Our main intention in presenting this\nperformance data is convey the relative speeds of the various crawlers.6.1. PerformanceOur production crawling machine is a Digital Ultimate Workstation with two 533 MHz Alpha processors, 2 GB of RAM,\n118 GB of local disk, and a 100 Mbit/sec FDDI connection to the Internet. We run Mercator under srcjava, a Java runtimedeveloped at our lab [10]. Running on this platform, a Mercator crawl run in May 1999 made 77.4 million HTTP requests in8 days, achieving an average download rate of 112 documents/sec and 1,682 KB/sec.\nThese numbers indicate that Mercator's performance compares favorably with that of the Google and the Internet Archive\ncrawlers. The Google crawler is reported to have issued 26 million HTTP requests over 9 days, averaging 33.5 docs/sec and200 KB/sec\u20ac[4]. This crawl was performed using four machines running crawler processes, and at least one more machinerunning the other processes. The Internet Archive crawler, which also uses multiple crawler machines, is reported to fetch 4million HTML docs/day, the average HTML page being 5KB\u20ac[21]. This download rate is equivalent to 46.3 HTML docs/secand 231 KB/sec. It is worth noting that Mercator fetches not only HTML pages, but documents of all other MIME types as\nwell. This effect more than doubles the size of the average document downloaded by Mercator as compared to the other\ncrawlers.\nAchieving the performance numbers described above required considerable optimization work. In particular, we spent a fairamount of time overcoming performance limitations of the Java core libraries\u20ac[13].We used DCPI, the Digital Continuous Profiling Infrastructure\u20ac[8], to measure where Mercator spends CPU cycles. DCPI isa freely available tool that runs on Alpha platforms. It can be used to profile both the kernel and user-level processes, and it\nprovides CPU cycle accounting at the granularity of processes, procedures, and individual instructions. We found that\nMercator spends 37% of its cycles in JIT-compiled Java bytecode, 19% in the Java runtime, and 44% in the Unix kernel.\nThe Mercator method accounting for the most cycles is the one that fingerprints the contents of downloaded documents\n(2.3% of all cycles).6.2. Selected Web StatisticsWe now present statistics collected about the web during the eight-day crawl described above.\nOne interesting statistic is the distribution of outcomes from HTTP requests. Roughly speaking, each URL removed from\nthe frontier causes a single HTTP request. However, there are two wrinkles to that equation, both related to the RobotsExclusion Protocol\u20ac[20]. First, before fetching a document, Mercator has to verify whether it has been excluded fromdownloading the document by the target site's robots.txt file. If the appropriate robots.txt data is not in Mercator's cache(described in Section\u20ac3.3), it must be downloaded, causing an extra HTTP request. Second, if the robots.txt file indicates thatMercator should not download the document, no HTTP request is made for the document. Table\u20ac1 relates the number of\nURLs removed from the frontier to the total number of HTTP requests.No. of URLs removed76,732,515\n+\u20ac\u20acNo. of robots.txt requests\u20ac\u20ac3,675,634\n-\u20ac\u20acNo. of excluded URLs3,050,768\n=\u20ac\u20acNo. of HTTP requests77,357,381\nTable 1: Relationship between the total number of URLs removed from the frontier and the total number ofHTTP requests.1.8 million of the 77.4 million HTTP requests (2.3%) did not result in a response, either because the host could not be\ncontacted or some other network failure occurred. Table\u20ac2 gives a breakdown of the HTTP status codes for the remainingMercator: A Scalable, Extensible Web Crawler\nhttp://www.research.compaq.com/SRC/mercator/papers/www/paper.html (11 de 14) [29/10/2003 16:33:31]75.6 million requests. We were somewhat surprised at the relatively low number of 404 status codes; we had expected to\ndiscover a higher percentage of broken links.Code\u20acMeaningNumberPercent\n200OK65,790,95387.03%\n404Not Found5,617,4917.43%\n302Moved Temporarily2,517,7053.33%\n301Moved Permanently842,8751.12%\n403Forbidden322,0420.43%\n401Unauthorized223,8430.30%\n500Internal Server Error83,7440.11%\n406Not Acceptable81,0910.11%\n400Bad Request65,1590.09%\nOther48,6280.06%\nTotal\u20ac\u20ac75,593,531\u20ac\u20ac100.0%\nTable 2:Breakdown of HTTP status codes.Of the 65.8 million documents that were successfully downloaded, 80% were between 1K and 32K bytes in size. Figure\u20ac2 is\na histogram showing the document size distribution. In this figure, the documents are distributed over 21 bins labeled with\nexponentially increasing document sizes; a document of size n is placed in the rightmost bin with a label not greater than n.Figure 2: Histogram of the sizes of successfully downloaded documents.According to our content-seen test, 8.5% of the successful HTTP requests (i.e., those with status code 200) were duplicates2.Of the 60 million unique documents that were successfully downloaded, the vast majority were HTML pages, followed in\npopularity by GIF and JPEG images. Table\u20ac3 shows the distribution of the most popular MIME types.MIME typeNumberPercent\ntext/html41,490,04469.2%\nimage/gif10,729,32617.9%\nimage/jpeg4,846,2578.1%\ntext/plain869,9111.5%\napplication/pdf540,6560.9%\naudio/x-pn-realaudio269,3840.4%\napplication/zip213,0890.4%\napplication/postscript159,8690.3%\nother829,4101.4%\nTotal\u20ac\u20ac59,947,946\u20ac\u20ac100.0%\nTable 3:Distribution of MIME types.Mercator: A Scalable, Extensible Web Crawler\nhttp://www.research.compaq.com/SRC/mercator/papers/www/paper.html (12 de 14) [29/10/2003 16:33:31]7. ConclusionsScalable web crawlers are an important component of many web services, but they have not been very well documented in\nthe literature. Building a scalable crawler is a non-trivial endeavor because the data manipulated by the crawler is too big to\nfit entirely in memory, so there are performance issues relating to how to balance the use of disk and memory. This paper\nhas enumerated the main components required in any scalable crawler, and it has discussed design alternatives for those\ncomponents.\nIn particular, the paper described Mercator, an extensible, scalable crawler written entirely in Java. Mercator's design\nfeatures a crawler core for handling the main crawling tasks, and extensibility through protocol and processing modules.\nUsers may supply new modules for performing customized crawling tasks. We have used Mercator for a variety of purposes,\nincluding performing random walks on the web, crawling our corporate intranet, and collecting statistics about the web at\nlarge.\nAlthough our use of Java as an implementation language was somewhat controversial when we began the project, we have\nnot regretted the choice. Java's combination of features --- including threads, garbage collection, objects, and exceptions ---\nmade our implementation easier and more elegant. Moreover, when run under a high-quality Java runtime, Mercator's\nperformance compares well to other web crawlers for which performance numbers have been published.\nMercator's scalability design has worked well. It is easy to configure the crawler for varying memory footprints. For\nexample, we have run it on machines with memory sizes ranging from 128 MB to 2 GB. The ability to configure Mercator\nfor a wide variety of hardware platforms makes it possible to select the most cost-effective platform for any given crawling\ntask.\nMercator's extensibility features have also been successful. As mentioned above, we have been able to adapt Mercator to a\nvariety of crawling tasks, and as stated earlier, the new code was typically quite small (tens to hundreds of lines). Java's\ndynamic class loading support meshes well with Mercator's extensibility requirements, and its object-oriented nature makes\nit easy to write variations of existing modules using subclassing. Writing new modules is also simplified by providing a\nrange of general-purpose reusable classes, such as classes for recording histograms and other statistics.Mercator is scheduled to be included in the next version of the AltaVista Search Intranet product\u20ac[1], a version of theAltaVista software sold mostly to corporate clients who use it to crawl and index their intranets.AcknowledgmentsThanks to Sanjay Ghemawat for providing the high-performance Java runtime we use for our crawls.Notes1 This problem has been corrected starting with BIND version 8.2.2 This figure ignores the successful HTTP requests that were required to fetch robots.txt files.References[1] AltaVista Software Search Intranet home page.http://altavista.software.digital.com/search/intranet\n[2] Berkeley Internet Name Domain (BIND).http://www.isc.org/bind.html\n[3] Burton Bloom. Space/time trade-offs in hash coding with allowable errors. Communications of the ACM, 13(7), pages422--426, July 1970.Mercator: A Scalable, Extensible Web Crawler\nhttp://www.research.compaq.com/SRC/mercator/papers/www/paper.html (13 de 14) [29/10/2003 16:33:31][4] Sergey Brin and Lawrence Page. The anatomy of a large-scale hypertextual Web search engine. In Proceedings of theSeventh International World Wide Web Conference, pages 107--117, April 1998.[5] Andrei Broder. Some Applications of Rabin's Fingerprinting Method. In R. Capocelli, A. De Santis, and U. Vaccaro,\neditors, Sequences II: Methods in Communications, Security, and Computer Science, pages 143--152. Springer-Verlag,1993.[6] Mike Burner. Crawling towards Eternity: Building an archive of the World Wide Web. Web Techniques Magazine, 2(5),May 1997.[7] Junghoo Cho, Hector Garcia-Molina, and Lawrence Page. Efficient crawling through URL ordering. In Proceedings ofthe Seventh International World Wide Web Conference, pages 161--172, April 1998.[8] Digital Continuous Profiling Infrastructure.http://www.research.digital.com/SRC/dcpi/[9] David Eichmann. The RBSE Spider -- Balancing Effective Search Against Web Load. In Proceedings of the FirstInternational World Wide Web Conference, pages 113--120, 1994.[10] Sanjay Ghemawat. srcjava home page.http://www.research.digital.com/SRC/java/\n[11] Google! Search Engine.http://google.stanford.edu/[12] Monika Henzinger, Allan Heydon, Michael Mitzenmacher, and Marc A. Najork. Measuring Index Quality usingRandom Walks on the Web. In Proceedings of the Eighth International World Wide Web Conference, pages 213--225, May1999.[13] Allan Heydon and Marc Najork. Performance Limitations of the Java Core Libraries. In Proceedings of the 1999 ACMJava Grande Conference, pages 35--41, June 1999.[14] The Internet Archive.http://www.archive.org/\n[15] The Web Robots Pages.http://info.webcrawler.com/mak/projects/robots/robots.html[16] Oliver A. McBryan. GENVL and WWWW: Tools for Taming the Web. In Proceedings of the First InternationalWorld Wide Web Conference, pages 79--90, 1994.[17] Robert C.\\ Miller and Krishna Bharat. SPHINX: A framework for creating personal, site-specific Web crawlers. InProceedings of the Seventh International World Wide Web Conference, pages 119--130, April 1998.[18] Brian Pinkerton. Finding What People Want: Experiences with the WebCrawler. In Proceedings of the SecondInternational World Wide Web Conference, 1994.[19] M. O. Rabin. Fingerprinting by Random Polynomials. Report TR-15-81, Center for Research in Computing\nTechnology, Harvard University, 1981.\n[20] The Robots Exclusion Protocol.http://info.webcrawler.com/mak/projects/robots/exclusion.html[21] Z. Smith. The Truth About the Web: Crawling towards Eternity. Web Techniques Magazine, 2(5), May 1997.[22] Internet Growth and Statistics: Credits and Background.http://www.mit.edu/people/mkgray/net/background.htmlLegal Statement Privacy StatementMercator: A Scalable, Extensible Web Crawler\nhttp://www.research.compaq.com/SRC/mercator/papers/www/paper.html (14 de 14) [29/10/2003 16:33:31]", "document_id": 9, "content_hash": "b83e698d9d2142e9df6a85b9a539609a"}